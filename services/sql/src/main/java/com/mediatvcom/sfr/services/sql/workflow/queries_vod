To join 2v-rx:

/*
Dataset<Row> sqlDF = spark.sql("SELECT ondemand_session_id_2v, count(ondemand_session_id_2v) as number " +
                 "From (" +
                 "SELECT rx.date as date_rx, 2v.date as date_2v, 2v.ondemand_session_id as ondemand_session_id_2v, " +
                 "2v.code_http as code_http_2v, rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                 "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx " +
                 "FROM srmRessource2v2cModel 2v " +
                 "LEFT JOIN usrm_vermserver_rx rx " +
                 "ON rx.ondemand_session_id = 2v.ondemand_session_id and rx.bitrate is not null " +
                 ") jointure " +
                 "Group BY ondemand_session_id_2v " +
                 "ORDER BY number DESC");
 */

To join 2v-rx-tx:

         Dataset<Row> sqlDF = spark.sql("SELECT ondemand_session_id_2v, count(ondemand_session_id_2v) as number " +
                 "From (" +
                 "SELECT rx.date as date_rx, 2v.date as date_2v, 2v.ondemand_session_id as ondemand_session_id_2v, " +
                 "2v.code_http as code_http_2v, rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                 "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx " +
                 "FROM srmRessource2v2cModel 2v " +
                 "LEFT JOIN usrm_vermserver_rx rx " +
                 "ON rx.ondemand_session_id = 2v.ondemand_session_id and rx.bitrate is not null " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id and tx.code != \"teardown_or_ok\"  and tx.code != \"error\"" +
                 ") jointure " +
                 "Group BY ondemand_session_id_2v " +
                 "ORDER BY number DESC");


3v-4v

         Dataset<Row> sqlDF = spark.sql("SELECT sessionId_3v, count(sessionId_3v) as number " +
                 "From( " +
                 "SELECT date as date_3v, " +
                 "session_id as sessionId_3v " +
                 "From srmSessionId3v5cModel 3v " +
                 "LEFT OUTER JOIN 4v " +
                 "ON session_id = 4v.sessionId_4v " +
                 ") jointure " +
                 "GROUP BY sessionId_3v " +
                 "ORDER BY number DESC");

3v-4v-5v

         Dataset<Row> sqlDF = spark.sql("SELECT sessionId_3v, count(sessionId_3v) as number " +
                 "FROM ( " +
                    "SELECT 3v.date as date_3v, " +
                    "3v.session_id as sessionId_3v, 3v.client_id, tsid_4v " +
                    "svcid_4v, content_type_4v, 5v.vip as vip_5v, " +
                    "5v.content_name as content_name_5v, 5v.content_type as content_type_5v " +
                    "From srmSessionId3v5cModel 3v " +
                    "LEFT JOIN 4v " +
                    "ON 3v.session_id = 4v.sessionId_4v " +
                    "LEFT JOIN srmSessionStart5vModel 5v " +
                    "ON 3v.session_id = 5v.session_id " +
                 ") jointure " +
                 "GROUP BY sessionId_3v " +
                 "ORDER BY number DESC").repartition(1);

3v-4v-5v-7v

         Dataset<Row> sqlDF = spark.sql("SELECT sessionId_3v, count(sessionId_3v) as number " +
                 "FROM ( " +
                    "SELECT 3v.date as date_3v, 3v.session_id as sessionId_3v, 3v.client_id, " +
                    "tsid_4v, svcid_4v, content_type_4v, " +
                    "5v.vip as vip_5v, 5v.content_name as content_name_5v, 5v.content_type as content_type_5v, " +
                    "7v.date as date_7v, 7v.session_id as sessionId_7v " +
                    "From srmSessionId3v5cModel 3v " +
                    "LEFT JOIN 4v " +
                    "ON 3v.session_id = 4v.sessionId_4v " +
                    "LEFT JOIN srmSessionStart5vModel 5v " +
                    "ON 3v.session_id = 5v.session_id " +
                    "LEFT JOIN srmEnd7v7cModel 7v " +
                    "ON 3v.session_id = 7v.session_id " +
                 ") jointure " +
                 "GROUP BY sessionId_3v " +
                 "ORDER BY number DESC ");


2v-rx-tx-3a:

Dataset<Row> sqlDF = spark.sql("SELECT rx.date as date_rx, 2v.date as date_2v, 2v.ondemand_session_id as ondemand_session_id_2v, " +
                                  "2v.code_http as code_http_2v, rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                                  "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx, " +
                                  "strm.date as date_s, strm.streamer as stream_s, strm.content_name as content_name_s," +
                                  "4c.content_type as content_type_c " +
                                  "FROM srmRessource2v2cModel 2v " +
                                  "LEFT JOIN usrm_vermserver_rx rx " +
                                  "ON rx.ondemand_session_id = 2v.ondemand_session_id and rx.bitrate is not null " +
                                  "LEFT JOIN usrm_vermserver_tx tx " +
                                  "ON rx.ondemand_session_id = tx.ondemand_session_id and tx.code != \"teardown_or_ok\"  and tx.code != \"error\" " +
                                  "LEFT JOIN streamerModel strm " +
                                  "ON strm.ip_rfgw = tx.ip_rfgw and strm.port_rfgw = tx.port_rfgw " +
                                  "and (unix_timestamp(strm.date, \"yyyy-MM-dd HH:mm:ss.SSS\") + 7200) >= unix_timestamp(tx.date, \"yyyy/MM/dd HH:mm:ss.SSS\") " +
                                  "and (unix_timestamp(strm.date, \"yyyy-MM-dd HH:mm:ss.SSS\") + 7200) < ( unix_timestamp(tx.date, \"yyyy/MM/dd HH:mm:ss.SSS\") + 30 )" +
                                  "LEFT JOIN srmSessionStart4cModel 4c " +
                                  "ON rx.ondemand_session_id = 4c.control_session " ).repartition(1);


1v-3v-3av-4v-5v-7v
Dataset<Row> sqlDF = spark.sql("SELECT 1v.date as date_1v, " +
                 "3av.date as date_3av, 3av.content_name as content_name_3av, 3av.carteId as carteId_3av, " +
                 "3v.date as date_3v, 3v.session_id as sessionId_3v, 3v.client_id as client_id_3v, " +
                 "tsid_4v, svcid_4v, content_type_4v, " +
                 "5v.vip as vip_5v, 5v.content_name as content_name_5v, 5v.content_type as content_type_5v, " +
                 "7v.date as date_7v, 7v.session_id as sessionId_7v " +
                 "From streamerModel 3av " +
                 "LEFT JOIN srmSessionId3v5cModel 3v " +
                 "ON 3av.cseq = 3v.cseq " +
                 "LEFT JOIN 4v " +
                 "ON 3v.session_id = 4v.sessionId_4v " +
                 "LEFT JOIN srmSessionStart5vModel 5v " +
                 "ON 3v.session_id = 5v.session_id " +
                 "LEFT JOIN srmEnd7v7cModel 7v " +
                 "ON 3v.session_id = 7v.session_id " +
                 "JOIN srmSetup1vModel 1v " +
                 "ON 1v.content_name = 3av.content_name and 1v.carteId = 3av.carteId  " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(1v.date, \"yyyy-MM-dd HH:mm:ss,SSS\") " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") < ( unix_timestamp(1v.date, \"yyyy-MM-dd HH:mm:ss,SSS\") + 60 ) ").repartition(1);


All VoD Workflow:


         Dataset<Row> sqlDF4v = spark.sql("SELECT MIN(date) as date_min_4v, MAX(date) as date_max_4v, " +
                 "4v.session_id as sessionId_4v, " +
                 "tsid as tsid_4v, svcid as svcid_4v, " +
                 "content_type as content_type_4v " +
                 "From srmTunningStartSession4V6v6S6CModel 4v " +
                 "Group by session_id, tsid, svcid, content_type").cache();

         sqlDF4v.createOrReplaceTempView("4v");


         Dataset<Row> sqlcommonDF = spark.sql("SELECT rx.date as date_rx, 2v.date as date_2v, 2v.ondemand_session_id as ondemand_session_id_2v, " +
                 "2v.code_http as code_http_2v, rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, tx.ip_rfgw as ip_rfgw_tx, " +
                 "tx.port_rfgw as port_rfgw_tx, rx.mode as mode_rx " +
                 "FROM srmRessource2v2cModel 2v " +
                 "LEFT JOIN usrm_vermserver_rx rx " +
                 "ON rx.ondemand_session_id = 2v.ondemand_session_id and rx.bitrate is not null " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id and tx.code != \"teardown_or_ok\"  and tx.code != \"error\" ").cache();

         sqlcommonDF.createOrReplaceTempView("common");

         Dataset<Row> sqlDF = spark.sql("SELECT 1v.date as date_1v, " +
                 "3av.port_rfgw as port_rfgw_3av, 3av.ip_rfgw as ip_rfgw_3av, " +
                 "3av.date as date_3av, 3av.content_name as content_name_3av, 3av.carteId as carteId_3av, " +
                 "3v.date as date_3v, 3v.session_id as sessionId_3v, 3v.client_id as client_id_3v, " +
                 "tsid_4v, svcid_4v, content_type_4v, " +
                 "5v.vip as vip_5v, 5v.content_name as content_name_5v, 5v.content_type as content_type_5v, " +
                 "7v.date as date_7v, 7v.session_id as sessionId_7v, " +
                 "common.port_rfgw_tx as port_rfgw_tx_common, common.ip_rfgw_tx as ip_rfgw_tx_common, common.bitrate_rx as bitrate_rx_common, common.date_rx as date_rx_common " +
                 "From streamerModel 3av " +
                 "LEFT JOIN srmSessionId3v5cModel 3v " +
                 "ON 3av.cseq = 3v.cseq " +
                 "LEFT JOIN 4v " +
                 "ON 3v.session_id = 4v.sessionId_4v " +
                 "LEFT JOIN srmSessionStart5vModel 5v " +
                 "ON 3v.session_id = 5v.session_id " +
                 "LEFT JOIN srmEnd7v7cModel 7v " +
                 "ON 3v.session_id = 7v.session_id " +
                 "JOIN srmSetup1vModel 1v " +
                 "ON 1v.content_name = 3av.content_name and 1v.carteId = 3av.carteId  " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(1v.date, \"yyyy-MM-dd HH:mm:ss,SSS\") " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") < ( unix_timestamp(1v.date, \"yyyy-MM-dd HH:mm:ss,SSS\") + 60 ) " +
                 "LEFT JOIN common " +
                 "ON common.port_rfgw_tx = 3av.port_rfgw and common.ip_rfgw_tx = 3av.ip_rfgw " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(common.date_2v, \"yyyy-MM-dd HH:mm:ss.SSS\") " +
                 "and unix_timestamp(3av.date, \"yyyy-MM-dd HH:mm:ss.SSS\") < ( unix_timestamp(common.date_2v, \"yyyy-MM-dd HH:mm:ss.SSS\") + 10 ) ").repartition(1);


Join 1v and 4v to get all issues (But not BW): We have FileName

         Dataset<Row> sqlDF = spark.sql("SELECT MIN(date_1v) as date_1v_gr, content_type_1v as content_type, date_4v as date_4v_gr, x_srm_error_message_4v as x_srm_error_message_4v_gr, " +
                 "carteId_4v as carteId_4v_gr, content_name_4v as content_name_4v_gr,  " +
                 "FROM ( " +
                 "SELECT 1v.date as date_1v, " +
                 "4v.date as date_4v, 4v.x_srm_error_message as x_srm_error_message_4v, 4v.carteId as carteId_4v, 4v.content_name as content_name_4v, 1v.content_type as content_type_1v " +
                 "FROM srmTunningStartSession4V6v6S6CModel 4v " +
                 "LEFT JOIN srmSetup1vModel 1v " +
                 "ON 1v.content_name = 4v.content_name and 1v.carteId = 4v.carteId " +
                 "and unix_timestamp(4v.date, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(1v.date, \"yyyy-MM-dd HH:mm:ss.SSS\" ) " +
                 "where 4v.error = \"true\" and 4v.content_name rlike '^[A-Z].*' " +
                 ") errors " +
                 "GROUP BY date_4v, x_srm_error_message_4v, carteId_4v, content_name_4v, content_type_1v ")
                 .repartition(1);

rx-tx: investigate the Not enough bandwidth

         SnmpTransfrom snmpTransfrom = new SnmpTransfrom(df_usrmSnmpModel);

         Dataset<Row> snmpTransformedDF = spark.createDataFrame(snmpTransfrom.getRowSnmpTransformedRDD(), usrmTransformedSnmpModel.getSchema());
         snmpTransformedDF.createOrReplaceTempView("snmp_transformed");

         Dataset<Row> sqlDF = spark.sql("SELECT rx.date as date_rx, rx.ondemand_session_id as ondemand_session_id_rx, " +
                 "rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                 "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx, tx.code_http as code_http_tx, tx.cseq as cseq_tx " +
                 "FROM usrm_vermserver_rx rx " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id " +
                 "where tx.code = \"error\" and tx.code_http = \"453 Not Enough Bandwidth\" " ).repartition(1);


We have: rfgw name
Not Enough bw:

         Dataset<Row> sql4verrorDF = spark.sql("SELECT rx.date as date_rx, rx.ondemand_session_id as ondemand_session_id_rx, " +
                 "rx.bitrate as bitrate_rx, regexp_extract(rx.service_group,\"(.+?_U).*\",1) as service_group_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                 "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx, tx.code_http as code_http_tx, tx.cseq as cseq_tx " +
                 "FROM usrm_vermserver_rx rx " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id " +
                 "where tx.code = \"error\" and tx.code_http = \"453 Not Enough Bandwidth\" " ).repartition(1);

         sql4verrorDF.createOrReplaceTempView("4verrors");

         Dataset<Row> sqlDF = spark.sql(" SELECT snmptf.*, 4verrors.* " +
                         "FROM snmp_transformed snmptf " +
                         "LEFT JOIN 4verrors " +
                         "WHERE snmptf.rfgw_id = 4verrors.service_group_rx " +
                         "and unix_timestamp(4verrors.date_rx, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(snmptf.date_down, \"yyyy-MM-dd HH:mm:ss.SSS\" ) " +
                         "and unix_timestamp(4verrors.date_rx, \"yyyy-MM-dd HH:mm:ss.SSS\") < unix_timestamp(snmptf.date_up, \"yyyy-MM-dd HH:mm:ss.SSS\" )" ).repartition(1);


All VOD Clear-Final:

         SnmpTransfrom snmpTransfrom = new SnmpTransfrom(df_usrmSnmpModel);

         // , cseq should be removed since it is somtimes set to some value!
         Dataset<Row> sqlDF4v = spark.sql("SELECT MIN(date) as date_min_4v, MAX(date) as date_max_4v, " +
                 "4v.session_id as sessionId_4v, " +
                 "tsid as tsid_4v, svcid as svcid_4v, " +
                 "content_type as content_type_4v " +
                 "From srmTunningStartSession4V6v6S6CModel 4v " +
                 "Group by session_id, tsid, svcid, content_type ").cache();

         sqlDF4v.createOrReplaceTempView("4v");

         Dataset<Row> sqlcommonDF = spark.sql("SELECT rx.date as date_rx, 2v.date as date_2v, 2v.ondemand_session_id as ondemand_session_id_2v, " +
                 "2v.code_http as code_http_2v, rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, tx.ip_rfgw as ip_rfgw_tx, " +
                 "tx.port_rfgw as port_rfgw_tx, rx.mode as mode_rx " +
                 "FROM srmRessource2v2cModel 2v " +
                 "LEFT JOIN usrm_vermserver_rx rx " +
                 "ON rx.ondemand_session_id = 2v.ondemand_session_id and rx.bitrate is not null " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id and tx.code != \"teardown_or_ok\"  and tx.code != \"error\" ").cache();

         sqlcommonDF.createOrReplaceTempView("common");


         Dataset<Row> sqlDF3a1v = spark.sql("SELECT MAX(1vus.date) as date_1vus, 1vus.content_type as content_type_1vus, 3aus.date as date_3aus, " +
                 "3aus.cseq as cseq_3aus, 3aus.client_id as client_id_3aus, 3aus.streamer as streamer_3aus, " +
                 "3aus.content_name as content_name_3aus, 3aus.carteId as carteId_3aus " +
                 "From streamerModel 3aus " +
                 "JOIN srmSetup1vModel 1vus " +
                 "ON 1vus.content_name = 3aus.content_name and 1vus.carteId = 3aus.carteId " +
                 "and unix_timestamp(3aus.date, \"yyyy-MM-dd HH:mm:ss.SSSSSS\") >= unix_timestamp(1vus.date, \"yyyy-MM-dd HH:mm:ss.SSS\") " +
                 "GROUP BY 3aus.client_id, 3aus.streamer, 3aus.cseq, 3aus.content_name, 3aus.date, 3aus.carteId, 1vus.content_type ").cache();

         sqlDF3a1v.createOrReplaceTempView("3aus1vus");

         Dataset<Row> sqlDF3aCommon = spark.sql("SELECT MAX(common.date_rx) as date_common, 3aus.date as date_3aus, 3aus.client_id as client_id_3aus, " +
                 "3aus.ip_rfgw as ip_rfgw_3aus, 3aus.port_rfgw as port_rfgw_3aus, 3aus.content_name as content_name_3aus, " +
                 "common.bitrate_rx as bitrate_rx_common, common.service_group_rx as service_group_rx_common, common.mode_rx as mode_rx_3aus " +
                 "From streamerModel 3aus " +
                 "JOIN common " +
                 "ON common.port_rfgw_tx = 3aus.port_rfgw and common.ip_rfgw_tx = 3aus.ip_rfgw " +
                 "and ( unix_timestamp(3aus.date, \"yyyy-MM-dd HH:mm:ss.SSSSSS\") + 7200 ) >= unix_timestamp(common.date_rx, \"yyyy-MM-dd HH:mm:ss.SSS\")  " +
                 "Group by 3aus.client_id, 3aus.cseq, common.bitrate_rx, common.service_group_rx, common.mode_rx, " +
                 "3aus.content_name, 3aus.date, 3aus.ip_rfgw, 3aus.port_rfgw ").cache();

         sqlDF3aCommon.createOrReplaceTempView("3auscommon");

         Dataset<Row> sqlDF = spark.sql("SELECT 3aus1vus.date_1vus as date_1vus_3aus, 3av.date as date_3av, 3av.port_rfgw as port_rfgw_3av, 3av.ip_rfgw as ip_rfgw_3av, " +
                 "3av.content_name as content_name_3av, 3av.carteId as carteId_3av, 3av.streamer as streamer_3av, " +
                 "3v.date as date_3v, 3v.session_id as sessionId_3v, 3v.client_id as client_id_3v, " +
                 "date_min_4v, tsid_4v, svcid_4v, " +
                 "5v.vip as vip_5v, 5v.content_name as content_name_5v, 5v.content_type as content_type_5v, " +
                 "7v.date as date_7v, 7v.session_id as sessionId_7v, " +
                 "3auscommon.service_group_rx_common as service_group_rx_common, 3auscommon.bitrate_rx_common as bitrate_rx , 3auscommon.mode_rx_3aus as mode_rx_3aus_common, " +
                 "3aus1vus.content_type_1vus as content_type " +
                 "From streamerModel 3av " +
                 "LEFT JOIN srmSessionId3v5cModel 3v " +
                 "ON 3av.cseq = 3v.cseq " +
                 "LEFT JOIN 4v " +
                 "ON 3v.session_id = 4v.sessionId_4v " +
                 "LEFT JOIN srmSessionStart5vModel 5v " +
                 "ON 3v.session_id = 5v.session_id " +
                 "LEFT JOIN srmEnd7v7cModel 7v " +
                 "ON 3v.session_id = 7v.session_id " +
                 "JOIN 3aus1vus " +
                 "ON 3aus1vus.content_name_3aus = 3av.content_name and 3aus1vus.client_id_3aus = 3av.client_id " +
                 "and 3aus1vus.cseq_3aus = 3av.cseq and 3aus1vus.date_3aus = 3av.date " +
                 "JOIN 3auscommon " +
                 "ON 3auscommon.ip_rfgw_3aus = 3av.ip_rfgw and 3auscommon.port_rfgw_3aus = 3av.port_rfgw " +
                 "and 3auscommon.date_3aus = 3av.date ").repartition(1);


VoD BW Error :

         SnmpTransfrom snmpTransfrom = new SnmpTransfrom(df_usrmSnmpModel);

         Dataset<Row> snmpTransformedDF = spark.createDataFrame(snmpTransfrom.getRowSnmpTransformedRDD(), usrmTransformedSnmpModel.getSchema());

         snmpTransformedDF.createOrReplaceTempView("snmp_transformed");

         Dataset<Row> sql4verrorDF = spark.sql("SELECT rx.date as date_rx, rx.ondemand_session_id as ondemand_session_id_rx, " +
                 "rx.bitrate as bitrate_rx, rx.service_group as service_group_rx, regexp_extract(rx.service_group,\"(.+?_U).*\",1) as service_group_detail_rx, rx.ip_rfgw as ip_rfgw_rx, " +
                 "rx.port_rfgw as port_rfgw_rx, rx.mode as mode_rx, tx.code_http as code_http_tx, tx.cseq as cseq_tx " +
                 "FROM usrm_vermserver_rx rx " +
                 "LEFT JOIN usrm_vermserver_tx tx " +
                 "ON rx.ondemand_session_id = tx.ondemand_session_id " +
                 "where rx.mode = \"unicast\" and tx.code = \"error\" and tx.code_http = \"453 Not Enough Bandwidth\" " ).repartition(1);

         sql4verrorDF.createOrReplaceTempView("4verrors");

         Dataset<Row> sqlsnmpDF = spark.sql(" SELECT snmptf.date_down as date_down_4vbw, snmptf.date_up as date_up_4vbw, " +
                 "4verrors.ondemand_session_id_rx as ondemand_session_id_rx_4v_4vbw " +
                 "FROM snmp_transformed snmptf " +
                 "LEFT JOIN 4verrors " +
                 "WHERE snmptf.rfgw_id = 4verrors.service_group_detail_rx " +
                 "and unix_timestamp(4verrors.date_rx, \"yyyy-MM-dd HH:mm:ss.SSS\") >= unix_timestamp(snmptf.date_down, \"yyyy-MM-dd HH:mm:ss.SSS\" ) " +
                 "and unix_timestamp(4verrors.date_rx, \"yyyy-MM-dd HH:mm:ss.SSS\") < unix_timestamp(snmptf.date_up, \"yyyy-MM-dd HH:mm:ss.SSS\" )" ).repartition(1);

         sqlsnmpDF.createOrReplaceTempView("snmp4v");

         Dataset<Row> sqlbwDF = spark.sql("SELECT 4verrors.date_rx, 4verrors.bitrate_rx, 4verrors.service_group_rx, 4verrors.ip_rfgw_rx, 4verrors.port_rfgw_rx, 4verrors.service_group_detail_rx, " +
                 "4verrors.code_http_tx, snmp4v.date_down_4vbw, snmp4v.date_up_4vbw, 4verrors.ondemand_session_id_rx, " +
                 "if (snmp4v.ondemand_session_id_rx_4v_4vbw = 4verrors.ondemand_session_id_rx, \"SRM Disconnected From USRM\", \"Not Enough Bandwidth\") as real_error_bw " +
                 "FROM 4verrors " +
                 "LEFT JOIN snmp4v " +
                 "ON snmp4v.ondemand_session_id_rx_4v_4vbw = 4verrors.ondemand_session_id_rx ").repartition(1);




Catchup content not found Error:

         Dataset<Row> sqlDF = spark.sql("SELECT * " +
                 "from srmGetContent0cModel 0c " +
                 "WHERE code_http = \"404 Not Found\"");

catchup several errors:

         Dataset<Row> sqlDF = spark.sql("SELECT * " +
                 "from srmPostContent1cModel " +
                 "WHERE code_http != \"200 OK\"");